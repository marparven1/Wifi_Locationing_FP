---
title: "Evaluate Techniques Wifi Locationing"
author: "Marta Venegas Pardo"
date: "2/28/2022"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE , warning = FALSE)
```

```{r}
library(dplyr)
library(kableExtra)

library(lubridate)
library(ggplot2)
```

# Descripción del proyecto

## Descripción

El proyecto se centra en el desarrollo de un sistema que se implementará en grandes campus industriales, en centros comerciales,.. con el fin de ayudar a las personas a navegar por un espacio interior y desconocido sin perderse.

Debemos investigar la posibilidad de usar "huellas dactilares wifi" para determinar la ubicación de una persona en espacios interiores (localización automática).

La toma de huellas WLAN utiliza las señales de múltiples puntos de acceso wifi dentro del edificio para determinar la ubicación, de manera análoga a cómo el GPS usa las señales satelitales.

La localización automática de usuarios consiste en estimar la posición del usuario (latitud, longitud y altitud) mediante el uso de un dispositivo electrónico, normalmente un teléfono móvil.

Sin embargo, la localización en interiores sigue siendo un problema abierto principalmente debido a la pérdida de señal GPS en entornos interiores. Aunque existen algunas tecnologías y metodologías de posicionamiento en interiores, esta base de datos se centra en las basadas en huellas dactilares WLAN (también conocidas como huellas dactilares WiFi).



## Objetivo: Ubicación en interiores

- El objetivo principal es la determinación de la posición física de una persona en un espacio interior de varios edificios mediante señales wifi.

- Variables objetivo: aquellas relacionadas con la posición del usuario, es decir, las coordenadas (latitud, longitud, piso) y el ID del edificio.


## Procedimiento

Se tratará de llevara cabo la localización de las personas mediante la creación de modelos de aprendizaje automático para ver cuál produce el mejor resultado.

Modelos:

- Regresión: latitud y longitud
- Clasificación: Piso e ID del edificio


El objetivo es estimar el edificio, el piso y las coordenadas (latitud y longitud) de las 1111 muestras incluidas en el conjunto de validación. Como también se incluyen los valores reales del edificio, planta y coordenadas, es posible determinar el error de localización.


# Lectura y descripción de los datos

## Descripción del conjunto de datos

Los datos han sido extraidos de la base de datos UJIIndoorLoc, que cubre tres edificios de la Universitat Jaume I en Castellón, Comunidad Valenciana, y éstos tienen 4 o 5 plantas, dependiendo del edificio y casi un total de 110.000m2. 

Fue creado en 2013 a través de más de 20 usuarios diferentes y 25 dispositivos Android (algún usuario usó más de un dispositivo). 

Existen 933 puntos de referencia en la base de datos

La base de datos consta de:

- 19937 registros de capacitación/referencia (archivo trainingData.csv)
- 1111 registros de validación/prueba (archivo validationData.csv)

Cada registro corresponde a una única medición y consta de 529 atributos contienen la huella digital WiFi WAP, las coordenadas donde se tomó y otra información útil como identificación del usuario o dispositivo móvil utilizado.

Cada huella WiFi se puede caracterizar por los puntos de acceso inalámbrico (WAP) detectados y la intensidad de la señal recibida (RSSI) correspondiente. 

Los valores de intensidad se representan como valores enteros negativos que van desde -104dBm (señal extremadamente pobre) hasta 0dbM. Por tanto, mientras más cercano sea el valor a 0, mejor será la señal.

El valor positivo 100 se ha usado para indicar cuándo no se detectó señal en un WAP concreto, sin embargo, posteriormente transformaremos este valor

Durante la creación de la base de datos, se detectaron 520 WAP diferentes. Por lo tanto, la huella dactilar WiFi está compuesta por 520 valores de intensidad.

También se registra información sobre quién (usuario), cómo (dispositivo y versión de Android) y cuándo (marca temporal) se tomó la captura WiFi.

## Variables incluídas en el dataset

-   WAP001-WAP520 (RSSI level): Valor de la intensidad de señal que llega al móvil desde cada punto de acceso inalámbrico WAP (Wireless Access Points) 1-520.

    -   Rango de valores: -104-0
    -   Unidad: decibelio-milivatio dBm. Se trata de una medida logarítmica de portencia en relación a un milivatio.
    -   Un valor de 100 dBm indica que la señal no fué detectada **
    -   Un valor a partir de -120 dBm indica que no se ha detectado señal
    -   Mientras más cercano sea elvalor a 0, mejor será la señal
    -   Nota: RSSI: Received Signal Strength Indicator

-   Longitude: longitud, variable numérica, con un rango de valores: desde -7695.9387549299299000 hasta -7299.786516730871000

-   Latitude: latitud, variable numérica, con un rango de valores: desde 4864745.7450159714 hasta 4865017.3646842018

-   Floor: Planta dentro del edificio, variable categórica (0-4)

-   BuildingID: Identidicador del edificio. Las medidas han sido tomadas en tres efidicios diferentes. Variable categórica (0-2)

    -   Listado:

        -   0 ESTCE - TI
        -   1 ESTCE - TD
        -   2 ESTCE - TC


-   SpaceID: Identificador del lugar donde se ha tomado la señal (oficina, pasillo o clase). Variable categórica (1-254)

-   RelativePosition: Posición relativa con respecto al espacio. Variable categórica (1 - dentro, 2 - Fuera, delante de la puerta).

-   UserID: Identificador del usuario, también encontramos la altura del usuario, ya que puede ser útil porque la posición espacial concreta del dispositivo tiene un impacto directo en los valores RSSI medidos. Variable categórica (0-18)


-   PhoneID: Identificador del modelo Android. Variable categórica (0-24)

    -   Listado:

        -   0 Celkon A27 4.0.4(6577) 0
        -   1 GT-I8160 2.3.6 8
        -   2 GT-I8160 4.1.2 0
        -   3 GT-I9100 4.0.4 5
        -   4 GT-I9300 4.1.2 0
        -   5 GT-I9505 4.2.2 0
        -   6 GT-S5360 2.3.6 7
        -   7 GT-S6500 2.3.6 14
        -   8 Galaxy Nexus 4.2.2 10
        -   9 Galaxy Nexus 4.3 0
        -   10 HTC Desire HD 2.3.5 18
        -   11 HTC One 4.1.2 15
        -   12 HTC One 4.2.2 0
        -   13 HTC Wildfire S 2.3.5 0,11
        -   14 LT22i 4.0.4 0,1,9,16
        -   15 LT22i 4.1.2 0
        -   16 LT26i 4.0.4 3
        -   17 M1005D 4.0.4 13
        -   18 MT11i 2.3.4 4
        -   19 Nexus 4 4.2.2 6
        -   20 Nexus 4 4.3 0
        -   21 Nexus S 4.1.2 0
        -   22 Orange Monte Carlo 2.3.5 17
        -   23 Transformer TF101 4.0.3 2
        -   24 bq Curie 4.1.1 12

-   Timestamp: Hora en que se tomó la señal. (Unidad: UNIX)





## Extracción de los datos

A continuación veremos el primer registro de los datos:


```{r}
Dat_Entrenamiento <- read.csv2(file = "Data_UJIndoorLoc/trainingData.csv",header = TRUE,sep = ",")
Dat_Validacion <- read.csv2(file = "Data_UJIndoorLoc/validationData.csv",header = TRUE,sep = ",")
# Dat_Entrenamiento %>% head()
# Dat_Entrenamiento[,-c(2:519)] %>% str()
# Dat_Entrenamiento %>% summary()
dim(Dat_Entrenamiento)
Dat_Entrenamiento[1,-c(2:519)] %>% kable(booktabs=TRUE) %>% kable_styling(latex_options = "striped")

```


La dimensión de los datos de entrenamiento, que son los que utilizaremos para tratar de modelar la intensidad de señal es  la siguiente: `r dim(Dat_Entrenamiento)` y la de los datos de validación de los modelos: `r dim(Dat_Validacion)`, y ambas tienen las mismas variables.







# Preprocesado de datos

## Valores faltantes

Ya nos indicaron en la descripción que no había datos faltantes.

## Outliers

## Varianza de la intensidad de señal

El conjunto de datos contiene un gran número de variables, con un total de `r ncol(Dat_Entrenamiento)`. Además, en el conjunto de las 520 variables predictoras, el valor predominante es el de la no detección de señal y esto hará que en muchos casos, la varianza de estas variables sea muy cercana a 0.



La varianza de 50 de las 520 variables las encontramos a continuación:

```{r}
apply(Dat_Entrenamiento[,1:520], 2, var) %>% head(100)
```
Nos planteamos ahora lo siguiente, ¿cuándo considerar una varianza muy pequeña?




El valor medio de todas las varianzas es de: `r round(apply(Dat_Entrenamiento[,1:520], 2, var) %>% mean(),2)`y los cuantiles: `r apply(Dat_Entrenamiento[,1:520], 2, var) %>% quantile()`. Es decir, el valor de la varianza que ha sido superado por el 75% de los WAPS es de 17.73. Podríamos considerar una varianza pequeña si no supera el valor del primer cuartil, 17.7394327, teniendo así `r length(which ( apply(Dat_Entrenamiento[,1:520], 2, var)  < 17.7394327 ))` variables con varianza menor que 10, siendo `r length(which ( apply(Dat_Entrenamiento[,1:520], 2, var)  == 0 ))` de ellas variables con varianza nula.


Eliminar WAPS es eliminar zonas de *detección*, pero a la hora del modelado hará que tengamos menos errores y por tanto, mayor fiabilidad en las métricas.

- Varianzas próximas a 0

Esto indicaría que existen zonas de los edificios que están poco cubiertas y por tanto, que se ha registrado conexión en muy pocas ocasiones y la mayoría de las veces se trata de registros de no detección. Esas zonas están muy poco transitadas y por ello, es necesario eliminar estas variables, ya que podrían introducir ruido.


- Varianza igual a 0

Para aquellas variables donde la varianza es nula (no es una variable, sino una constante) o con un valor menor que el umbral, optaremos por eliminarlas de nuestro conjunto de datos para así reducir el número de variables. 

Si la varianza de un WAP es nula, eso indicaría que no se pasa por ningún área de influencia de ese WAP, es decir, nadie ha pasado por ahí y hay un área que no está cubierta. Sin embargo, no podemos saber de que área se trata ya que no sabemos donde están ubicados los diferentes WAPs. Es necesario eliminar éstas variables ya que al haber partes del edificio que no registran señal, no aportan información y por tanto nuestro modelo no sería del todo adecuado.

En nuestro conjunto de datos, hay un total de `r length(which ( (apply(Dat_Entrenamiento[,1:520], 2, var) == 0 ) ) )`  WAPs  que tienen varianza 0 de los 520 existentes, y son las siguientes mostradas a continuación:


```{r}
which ( apply(Dat_Entrenamiento[,1:520], 2, var) == 0 )
```


Esto implicaría que ahora tenemos `r 520 - length(which ( (apply(Dat_Entrenamiento[,1:520], 2, var) < 10 ) ) )` variables que miden la intensidad de señal.


```{r}
Dat_EntrenamientoII <- Dat_Entrenamiento[,-which ( apply(Dat_Entrenamiento[,1:520], 2, var) < 17.7394327 )  ]

```

Ahora tenemos un conjunto de dato con las siguientes dimensiones: `r dim(Dat_EntrenamientoII)`.












## Duplicados



Existen un total de `r length(which(duplicated(Dat_EntrenamientoII[,])))` registros duplicados (idénticos) y supone un `r round( 100*(length(which(duplicated(Dat_EntrenamientoII[,])))/nrow(Dat_EntrenamientoII)),2)` % del total de datos, por lo que al ser un porcentaje tan bajo, las instancias serán eliminadas ya que podrían influir en nuestro análisis y modelado posterior.

Nota: En esta primera fase del tratamiento, hemos considerado que dos o más filas están duplicadas si estaban duplicadas en su totalidad, es decir, dos o más filas filas con los mismos atributos, no solo de identificación, sino también la misma marca temporal e intensidad de señal. Las 529 variables eran idénticas para estos registros.

```{r}
Dat_EntrenamientoIII<- Dat_EntrenamientoII[-which(duplicated(Dat_EntrenamientoII[,])),]
```


Trás este procedimiento, obtenemos un conjunto de datos con las siguientes dimensiones:  `r dim(Dat_EntrenamientoIII)`


A continuación, trás haber procedido a eliminar aquellas filas que estaban duplicadas en su totalidad, vamos a centrarnos en buscar registros duplicados considerando por éstos aquellas instancias que hayan sido registrados por el mismo individuo en la misma fecha (día y hora) y con el mismo teléfono móvil.



```{r}
Duplicados <- Dat_EntrenamientoIII[which(duplicated(Dat_EntrenamientoIII[,c("USERID","PHONEID","TIMESTAMP")])) ,] 
```


Encontramos un total de `r dim(Duplicados)[1]` registros tomados por el mismo usuario con el mismo dispositivo y con la misma marca horaria. Esto supone un `r round((dim(Duplicados)[1]*100)/dim(Dat_EntrenamientoIII)[1],2)` % del total de registros, que es un porcentaje alto.


Los registros duplicados, y más un número tan elevado respecto del total de número de instancias podrían influir en los resultados de los modelos predictivos que vayamos a construir, y por tanto influiría directamente en el resultado que obtendremos de nuestro análisis.

¿Qué hacer y qué implicaría quitarlos?


Vamos a optar por eliminar estos registros duplicados, ya que podrían tratarse de registros erróneos en la toma de datos y que podrían llevarnos a construir modelos predictivos de menor calidad y que no saquen el mayor partido a los datos disponibles.

Eliminar estos registros implica perder información pero ganar consistencia en los datos y por tanto, en los resultados.

```{r}
Dat_EntrenamientoIV <- 
Dat_EntrenamientoIII[-which(duplicated(Dat_EntrenamientoIII[,c("USERID","PHONEID","TIMESTAMP")])) ,] 
```

Por tanto, ahora tenemos un conjunto de datos con las siguientes dimensiones: `r dim(Dat_EntrenamientoIV)`



## Transformación de variables

En este apartado, transformaremos las variables para que tengan el formato y/o unidad oportunos para su posterior estudio.

### Categorización

Debemos categorizar las variables oportunas, como aquellas correspondientes a ID, posición relativa y planta.

```{r}
Dat_EntrenamientoIV$FLOOR <- as.factor(Dat_EntrenamientoIV$FLOOR)
Dat_EntrenamientoIV$BUILDINGID<-as.factor(Dat_EntrenamientoIV$BUILDINGID)
Dat_EntrenamientoIV$SPACEID<-as.factor(Dat_EntrenamientoIV$SPACEID)
Dat_EntrenamientoIV$RELATIVEPOSITION<-as.factor(Dat_EntrenamientoIV$RELATIVEPOSITION)
Dat_EntrenamientoIV$USERID<-as.factor(Dat_EntrenamientoIV$USERID)
Dat_EntrenamientoIV$PHONEID<-as.factor(Dat_EntrenamientoIV$PHONEID)
```



Las variables latitud y longitud deben tener formato numérico, no de factor.

```{r}
Dat_EntrenamientoIV$LATITUDE<-as.numeric(Dat_EntrenamientoIV$LATITUDE)
Dat_EntrenamientoIV$LONGITUDE<-as.numeric(Dat_EntrenamientoIV$LONGITUDE)
```



```{r}
Dat_EntrenamientoIV[,-c(1:389)] %>% str()
```




```{r}
# Dat_EntrenamientoIV[,-c(1:390)] %>% summary()
```


### Reformateo de la no detección de señal

Nos han indicado que la no cobertura se ha caracterizado en nuestro dataset con un señal de 100 dBm. Como esto no es una situación real, vamos a transformar los registros con que tengan una señal con un valor de 100 dBm a un valor más pequeño que indique que no se ha registrado señal en ese WAP.

El valor de intensidad de señal mínimo registrado ha sido de `r min(apply(Dat_EntrenamientoIV[,1:390], 1, min))` dBm, por tanto, optaremos por considerar que el valor de la no cobertura para la no conexión es de -120 dBm.



Realizamos la transformación y mostramos unos cuantos registros para su visualización.

```{r}
for (i in seq_len(389)) {
 Dat_EntrenamientoIV[,i] =  ifelse(Dat_EntrenamientoIV[,i]==100, -120 , Dat_EntrenamientoIV[,i] )
}

Dat_EntrenamientoIV[,-c(7:388)] %>% head(3)

```


### Formato horario


Además, tenemos que transformar la variable Timestamp a formato horario.



```{r}
Dat_EntrenamientoIV$TIMESTAMP<-as_datetime(Dat_EntrenamientoIV$TIMESTAMP)

Dat_EntrenamientoIV$year <-    year( as_datetime(Dat_EntrenamientoIV$TIMESTAMP) ) # extraemos el año
Dat_EntrenamientoIV$month<-  month(as_datetime(Dat_EntrenamientoIV$TIMESTAMP) ) # extraemos el mes
Dat_EntrenamientoIV$day<-       day(as_datetime(Dat_EntrenamientoIV$TIMESTAMP)  ) # extraemos el día
Dat_EntrenamientoIV$hour<-     hour( as_datetime(Dat_EntrenamientoIV$TIMESTAMP)  ) # extraemos la hora
Dat_EntrenamientoIV$week<-     week( as_datetime(Dat_EntrenamientoIV$TIMESTAMP) )
Dat_EntrenamientoIV$weekday<-wday(   as_datetime(Dat_EntrenamientoIV$TIMESTAMP),week_start = 1)



Dat_EntrenamientoIV$month   <- as.factor(Dat_EntrenamientoIV$month  )
Dat_EntrenamientoIV$day     <- as.factor(Dat_EntrenamientoIV$day    )
Dat_EntrenamientoIV$hour    <- as.factor(Dat_EntrenamientoIV$hour   )
Dat_EntrenamientoIV$week    <- as.factor(Dat_EntrenamientoIV$week   )
Dat_EntrenamientoIV$weekday <- as.factor(Dat_EntrenamientoIV$weekday)


 Dat_EntrenamientoIV$TIMESTAMP<-  as.POSIXct(Dat_EntrenamientoIV$TIMESTAMP,origin=  "1970-01-01")
 # attr(Dat_EntrenamientoIV$TIMESTAMP, "tzone") <- "Europe/Paris"
```












### Creación de nuevas variables

Es interesante conocer para cada instancia registrada por un usuario, cual es el WAP que tiene la mayor intensidad de señal.

Por tanto, crearemos dos nuevas variables:

- SeñalMax: valor de la intensidad de señal máxima registrada para cada instancia

- Señal min: valor de la intensidad mínima de señal registrada para cada instancia

- NumWAP_Detectados: Para cada registro, se va a recoger el número de WAPS detectados 


```{r}
Dat_EntrenamientoIV$MaxSenal <- apply( Dat_EntrenamientoIV[, 1:389], 1, function(x)
  max(x))
Dat_EntrenamientoIV$MinSenal <- apply( Dat_EntrenamientoIV[, 1:389], 1, function(x)
  min(x[x > -120]))
Dat_EntrenamientoIV$NumWAP_Detectados <- apply( Dat_EntrenamientoIV[, 1:389], 1, function(x)
  sum(x > -120))

```


El conjunto de datos tiene las siguientes dimensiones: 

- Número de filas: `r dim(Dat_EntrenamientoIV)[1]`
- Número de columnas `r dim(Dat_EntrenamientoIV)[2]` de las cuales 389 corresponden a la intensidad de señal detectada



Vamos a estudiar con detenimiento el número de WAPS detectados, ya que es fundamental tener en cuenta que hay 389 variables WAP y lo mas probable es que la mayoría de estos valores correspondan a la no detección de señal.

- Valor medio, mediano y quartiles:

```{r}
Dat_EntrenamientoIV$NumWAP_Detectados %>% summary()
```


En el 50% del número total de registros se ha detectado señal con un total de 16 WAPS o menos al mismo tiempo.

- Deciles:

```{r}
Dat_EntrenamientoIV$NumWAP_Detectados %>% quantile(prob=seq(0, 1, length = 11))
```


```{r}
ggplot(data=Dat_EntrenamientoIV , aes(x=NumWAP_Detectados ) ) + 
 # scale_y_continuous(breaks = seq(0,3000,by=500) )+
    geom_bar(stat="count", position="dodge2", fill="#FF6666")+ # position=position.stack se puede abreviar con position="stack".
labs(title = "Sistribución de la frecuencia de WAPS detectados") + 
  xlab("Nº WAPS detectados")+
  ylab("Num registros en la base de datos")+
  scale_x_continuous(breaks = seq(0,51,by=5))+
  scale_y_continuous(breaks = seq(0,1200,by=100))
```


Vemos como algunas mediciones no han detectado ningún punto de acceso y el valor máximo de puntos de acceso detectados es de 51 WAPS en total. Esto indicaría que el resto de los `r 389 - 51` valores de la huella digital indican la no detección, es decir, conexión nula.  Es decir, si el 90% de las mediciones han detectado únicamente 28 puntos de acceso o menos, la mayoría de los valores de nuestra base de datos son nulos.

En primer lugar, eliminamos aquellas filas que no hayan registrado intensidad de señal con ningún WAP, ya que esto significa que a esa zona no llega conexión alguna. Se trata de un total de  `r nrow(Dat_EntrenamientoIV[Dat_EntrenamientoIV$NumWAP_Detectados == 0,])` registros de los usuarios.

```{r}
Dat_EntrenamientoV<-Dat_EntrenamientoIV %>% filter(Dat_EntrenamientoIV$NumWAP_Detectados != 0)
```



Podríamos optar por reducir el número de valores nulos en nuestra base datos y para ello, habría que hacer lo siguiente: seleccionar únicamente aquellos WAPs con los que se haya registrado una intensidad de señal mayor, haciendo uso de la nueva variable llamada MaxSenal, que indica cual ha sido la máxima intensidad de señal registrada en cada medición. 

Sin embargo, si hicéramos esto únicamente nos quedaríamos con `r length( unique( unlist(MiLista) )  )` WAPS de los 520 iniciales, pero eso sería perder demasiada información, y por tanto, nos quedaremos como estamos actualmente, con un total de 389 WAPS de los 520 iniciales.



```{r}
MiLista<-as.list(as.list(rep(NA,389))) 

for (i in seq_len(389)) {
  MiLista[[i]]<- colnames( Dat_EntrenamientoV[i,Dat_EntrenamientoV[i,]== Dat_EntrenamientoV$MaxSenal[i]])
  MiLista[[i]]<- MiLista[[i]][-length(MiLista[[i]])]
}
# En esta lista contenemos los WAPS para los que tenemos el valor máximo de señal

# length( unique( unlist(MiLista) )  )
# Hay 39 diferentes, hablar con Mercé, son muy pocos WAPS -> PIERDO MUCHA INFORMACIÓN
# Son los WAPS ppales
```







## Conjunto de datos trás el data cleaning


A continuación mostraremos el formato de las variables del conjunto de datos final, con una breve visualización de las mismas


```{r}
Dat_EntrenamientoV[,-c(2:389)] %>% str()
```



```{r}
DT::datatable(Dat_EntrenamientoV[,-c(7:4387)])
```


Por último, guardamos los datos ya preparados para el análisis en un archivo tipo *.RData* llamado *TrainingDataCleaned.RData*

```{r}
TrainingDataCleaned<- Dat_EntrenamientoV
save(TrainingDataCleaned,file = "Data_UJIndoorLoc/TrainingDataCleaned.RData")
```


